{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Insult_pred.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7--ayZ74FSlP"
      },
      "source": [
        "# **Detectar insultos en redes sociales.**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfqIGoOBFbDI"
      },
      "source": [
        "import re, string\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNnS87kbHjO0"
      },
      "source": [
        "**Load raw data**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2bZ9SDOIyn5",
        "outputId": "36fed25f-557a-4ba6-f999-e102b0c8cfc4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HpODnmJI9d5",
        "outputId": "5fc1ce15-18c6-4d51-f5a7-c4deaf45acee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "%cd '/content/drive/My Drive/Colab Notebooks/db'\n",
        "!ls"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/db\n",
            "insults.csv    london_merged.csv  Mall_Customers.csv\t  titanic.csv\n",
            "insurance.csv  LR_ML.xlsx\t  Meteorite_Landings.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LN3LbCgHT_M"
      },
      "source": [
        "def clean_text(text):\n",
        "  text = text.lower()\n",
        "  text = re.findall(r'\\b[a-z]+\\b', text)\n",
        "  return ' '.join(text)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtspEQaPH-mE",
        "outputId": "e1379657-7e90-4699-a76d-9e6967504f5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "training_data = pd.read_csv('insults.csv')\n",
        "training_data.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Insult</th>\n",
              "      <th>Date</th>\n",
              "      <th>Comment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>20120618192155Z</td>\n",
              "      <td>\"You fuck your dad.\"</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>20120528192215Z</td>\n",
              "      <td>\"i really don't understand your point.\\xa0 It ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>\"A\\\\xc2\\\\xa0majority of Canadians can and has ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>\"listen if you dont wanna get married to a man...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>20120619094753Z</td>\n",
              "      <td>\"C\\xe1c b\\u1ea1n xu\\u1ed1ng \\u0111\\u01b0\\u1edd...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Insult             Date                                            Comment\n",
              "0       1  20120618192155Z                               \"You fuck your dad.\"\n",
              "1       0  20120528192215Z  \"i really don't understand your point.\\xa0 It ...\n",
              "2       0              NaN  \"A\\\\xc2\\\\xa0majority of Canadians can and has ...\n",
              "3       0              NaN  \"listen if you dont wanna get married to a man...\n",
              "4       0  20120619094753Z  \"C\\xe1c b\\u1ea1n xu\\u1ed1ng \\u0111\\u01b0\\u1edd..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wETQe3rJPLQ",
        "outputId": "d925e2ca-95ff-419f-e268-f850d14f69cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "training_data['cleaned_comment'] = training_data['Comment'].map(clean_text)\n",
        "training_data.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Insult</th>\n",
              "      <th>Date</th>\n",
              "      <th>Comment</th>\n",
              "      <th>cleaned_comment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>20120618192155Z</td>\n",
              "      <td>\"You fuck your dad.\"</td>\n",
              "      <td>you fuck your dad</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>20120528192215Z</td>\n",
              "      <td>\"i really don't understand your point.\\xa0 It ...</td>\n",
              "      <td>i really don t understand your point it seems ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>\"A\\\\xc2\\\\xa0majority of Canadians can and has ...</td>\n",
              "      <td>a of canadians can and has been wrong before n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>\"listen if you dont wanna get married to a man...</td>\n",
              "      <td>listen if you dont wanna get married to a man ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>20120619094753Z</td>\n",
              "      <td>\"C\\xe1c b\\u1ea1n xu\\u1ed1ng \\u0111\\u01b0\\u1edd...</td>\n",
              "      <td>c b xu bi t xecnh c ho kh nc ng d ng cu xed ch...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Insult  ...                                    cleaned_comment\n",
              "0       1  ...                                  you fuck your dad\n",
              "1       0  ...  i really don t understand your point it seems ...\n",
              "2       0  ...  a of canadians can and has been wrong before n...\n",
              "3       0  ...  listen if you dont wanna get married to a man ...\n",
              "4       0  ...  c b xu bi t xecnh c ho kh nc ng d ng cu xed ch...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tU6ZOXmjJxtY"
      },
      "source": [
        "**Make some festures**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbmaP56pJj23",
        "outputId": "c149cea3-40d5-47ca-952a-948285f4a20b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "count_vectorizer = CountVectorizer(analyzer='word', ngram_range=(1, 3), stop_words='english', max_features=50000)\n",
        "count_vectorizer.fit(training_data['cleaned_comment'])"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
              "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
              "                lowercase=True, max_df=1.0, max_features=50000, min_df=1,\n",
              "                ngram_range=(1, 3), preprocessor=None, stop_words='english',\n",
              "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                tokenizer=None, vocabulary=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNIEjlGGKlPO"
      },
      "source": [
        "X = count_vectorizer.transform(training_data['cleaned_comment'])\n",
        "y = training_data['Insult']"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZ2rh_yFLBJ_"
      },
      "source": [
        "**Cross-Validate**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLDOOaFrLIJE"
      },
      "source": [
        "*Split-data*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-4YU6W9KzoX"
      },
      "source": [
        "mask = [bool(np.random.binomial(1, .75)) for _ in range(X.shape[0])]\n",
        "mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uX1kFCi1Li4V",
        "outputId": "4f6375bf-1b6c-4ffc-ea0d-b536e2b59628",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X[np.array(mask)]\n",
        "sum(mask)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2945"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GypxvtlaL5qJ",
        "outputId": "03361f06-6063-4bcf-9d22-e854e384c389",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y[mask].shape[0]"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2945"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJrxioekNKH9"
      },
      "source": [
        "y[~mask].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSRAWTUNMByB"
      },
      "source": [
        "def split_data(X, y, p= 0.75):\n",
        "  mask = np.array([bool(np.random.binomial(1, .75)) for _ in range(X.shape[0])])\n",
        "\n",
        "  X_train = X[mask]\n",
        "  y_train = y[mask]\n",
        "  X_validation = X[~mask]\n",
        "  y_validation = y[~mask]\n",
        "\n",
        "  return X_train, y_train, X_validation, y_validation"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zR5u51KMPU7x"
      },
      "source": [
        "X_train, y_train, X_validation, y_validation = split_data(X, y)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpAPQZvmOAbV"
      },
      "source": [
        "*Fit a model on training data*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fBkyd0fMJDT",
        "outputId": "565bd987-b56a-4d15-bbaa-741b3873e904",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZ8tDs--OeUu"
      },
      "source": [
        "*Validate model on validation data*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Ma_65tpOT3f",
        "outputId": "fceecbeb-2ea6-4488-b4f3-9a5aac47adc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "predictions = model.predict(X_validation)\n",
        "baseline_pred = np.zeros(predictions.shape[0])\n",
        "validation_score = accuracy_score(y_validation, predictions)\n",
        "\n",
        "print('Validation Score: ', validation_score)\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation Score:  0.8189823874755382\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpzBhYQAPNWj",
        "outputId": "8fc6c236-8b3a-4056-80a8-461556bdb759",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "baseline_validation_score = accuracy_score(y_validation, baseline_pred)\n",
        "print('Validation score: ', baseline_validation_score)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation score:  0.7318982387475538\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3awx7uuRigj"
      },
      "source": [
        "**Everything is a hyper-parameter**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zeAHn_7Pzpe"
      },
      "source": [
        "class PredictionPipeline:\n",
        "\n",
        "  def __init__(self, ngram_range, vectorizer_class, model_class, training_data):\n",
        "    self.ngram_range = ngram_range\n",
        "    self.vectorizer_class = vectorizer_class\n",
        "    self.model_class = model_class\n",
        "    self.training_data = training_data\n",
        "    self.vectorizer = None\n",
        "    self.X = None\n",
        "    self.y = None\n",
        "    self.model = None\n",
        "    self.validation_score = None\n",
        "\n",
        "  def run(self):\n",
        "    self._fit_vectorizer()\n",
        "    self._featurize_text()\n",
        "    self._split_train_and_validation_sets()\n",
        "    self._fit_model_on_training_data()\n",
        "    self._validate_model_on_validation_set()\n",
        "\n",
        "    print(\n",
        "            \"\"\"\n",
        "            Vectorizer Class: {vectorizer_class}\\n\\\n",
        "            N-gram Range: {ngram_range}\\n\\\n",
        "            Model Class: {model_class}\\n\\\n",
        "            Validation Score: {validation_score}\n",
        "            \"\"\".format(\n",
        "\n",
        "            vectorizer_class=repr(self.vectorizer_class.__name__), \n",
        "            ngram_range=self.ngram_range, \n",
        "            model_class=repr(self.model_class.__name__), \n",
        "            validation_score=round(self.validation_score, 4)\n",
        "\n",
        "            )\n",
        "            )\n",
        "  def _fit_vectorizer(self):\n",
        "    self.vectorizer = vectorizer_class(analyzer='word', ngram_range=ngram_range, \n",
        "                                     stop_words='english', max_features=50000)\n",
        "    self.vectorizer.fit(self.training_data['cleaned_comment'])\n",
        "\n",
        "  def _featurize_text(self):\n",
        "    self.X = self.vectorizer.transform(self.training_data['cleaned_comment'])\n",
        "    self.y = self.training_data['Insult']\n",
        "\n",
        "  def _split_train_and_validation_sets(self):\n",
        "    self.X_train, self.y_train, self.X_validation, self.y_validation = split_data(\n",
        "            self.X, self.y)\n",
        "    \n",
        "  def _fit_model_on_training_data(self):\n",
        "    self.model = self.model_class()\n",
        "    self.model.fit(self.X_train, self.y_train)\n",
        "\n",
        "  def _validate_model_on_validation_set(self):\n",
        "    predictions = self.model.predict(self.X_validation)\n",
        "    self.validation_score = accuracy_score(self.y_validation, predictions)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYK4oMa1qgNC",
        "outputId": "7d38f19b-15d9-49c5-8a01-1ca511849b78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "results = {}\n",
        "\n",
        "for ngram_range in [(1, 1), (1, 2), (1, 3), (1, 4)]:\n",
        "  for vectorizer_class in [CountVectorizer, TfidfVectorizer]:\n",
        "    for model_class in [LogisticRegression, LinearSVC, RandomForestClassifier]:\n",
        "\n",
        "      # run prediction pipeline\n",
        "      prediction_pipeline = PredictionPipeline(\n",
        "          ngram_range=ngram_range,\n",
        "          vectorizer_class=vectorizer_class,\n",
        "          model_class=model_class,\n",
        "          training_data=training_data\n",
        "            )\n",
        "      \n",
        "      prediction_pipeline.run()\n",
        "\n",
        "      # add hyper-parameters to `results` dictionary\n",
        "      results[str(prediction_pipeline.validation_score)] = {\n",
        "              'vectorizer_class': prediction_pipeline.vectorizer_class,\n",
        "              'ngram_range': prediction_pipeline.ngram_range,\n",
        "              'model_class': prediction_pipeline.model_class\n",
        "            }\n"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "            Vectorizer Class: 'CountVectorizer'\n",
            "            N-gram Range: (1, 1)\n",
            "            Model Class: 'LogisticRegression'\n",
            "            Validation Score: 0.8374\n",
            "            \n",
            "\n",
            "            Vectorizer Class: 'CountVectorizer'\n",
            "            N-gram Range: (1, 1)\n",
            "            Model Class: 'LinearSVC'\n",
            "            Validation Score: 0.8241\n",
            "            \n",
            "\n",
            "            Vectorizer Class: 'CountVectorizer'\n",
            "            N-gram Range: (1, 1)\n",
            "            Model Class: 'RandomForestClassifier'\n",
            "            Validation Score: 0.813\n",
            "            \n",
            "\n",
            "            Vectorizer Class: 'TfidfVectorizer'\n",
            "            N-gram Range: (1, 1)\n",
            "            Model Class: 'LogisticRegression'\n",
            "            Validation Score: 0.7889\n",
            "            \n",
            "\n",
            "            Vectorizer Class: 'TfidfVectorizer'\n",
            "            N-gram Range: (1, 1)\n",
            "            Model Class: 'LinearSVC'\n",
            "            Validation Score: 0.8326\n",
            "            \n",
            "\n",
            "            Vectorizer Class: 'TfidfVectorizer'\n",
            "            N-gram Range: (1, 1)\n",
            "            Model Class: 'RandomForestClassifier'\n",
            "            Validation Score: 0.8266\n",
            "            \n",
            "\n",
            "            Vectorizer Class: 'CountVectorizer'\n",
            "            N-gram Range: (1, 2)\n",
            "            Model Class: 'LogisticRegression'\n",
            "            Validation Score: 0.8217\n",
            "            \n",
            "\n",
            "            Vectorizer Class: 'CountVectorizer'\n",
            "            N-gram Range: (1, 2)\n",
            "            Model Class: 'LinearSVC'\n",
            "            Validation Score: 0.8184\n",
            "            \n",
            "\n",
            "            Vectorizer Class: 'CountVectorizer'\n",
            "            N-gram Range: (1, 2)\n",
            "            Model Class: 'RandomForestClassifier'\n",
            "            Validation Score: 0.8317\n",
            "            \n",
            "\n",
            "            Vectorizer Class: 'TfidfVectorizer'\n",
            "            N-gram Range: (1, 2)\n",
            "            Model Class: 'LogisticRegression'\n",
            "            Validation Score: 0.7329\n",
            "            \n",
            "\n",
            "            Vectorizer Class: 'TfidfVectorizer'\n",
            "            N-gram Range: (1, 2)\n",
            "            Model Class: 'LinearSVC'\n",
            "            Validation Score: 0.7891\n",
            "            \n",
            "\n",
            "            Vectorizer Class: 'TfidfVectorizer'\n",
            "            N-gram Range: (1, 2)\n",
            "            Model Class: 'RandomForestClassifier'\n",
            "            Validation Score: 0.8121\n",
            "            \n",
            "\n",
            "            Vectorizer Class: 'CountVectorizer'\n",
            "            N-gram Range: (1, 3)\n",
            "            Model Class: 'LogisticRegression'\n",
            "            Validation Score: 0.8204\n",
            "            \n",
            "\n",
            "            Vectorizer Class: 'CountVectorizer'\n",
            "            N-gram Range: (1, 3)\n",
            "            Model Class: 'LinearSVC'\n",
            "            Validation Score: 0.8244\n",
            "            \n",
            "\n",
            "            Vectorizer Class: 'CountVectorizer'\n",
            "            N-gram Range: (1, 3)\n",
            "            Model Class: 'RandomForestClassifier'\n",
            "            Validation Score: 0.8124\n",
            "            \n",
            "\n",
            "            Vectorizer Class: 'TfidfVectorizer'\n",
            "            N-gram Range: (1, 3)\n",
            "            Model Class: 'LogisticRegression'\n",
            "            Validation Score: 0.7776\n",
            "            \n",
            "\n",
            "            Vectorizer Class: 'TfidfVectorizer'\n",
            "            N-gram Range: (1, 3)\n",
            "            Model Class: 'LinearSVC'\n",
            "            Validation Score: 0.8204\n",
            "            \n",
            "\n",
            "            Vectorizer Class: 'TfidfVectorizer'\n",
            "            N-gram Range: (1, 3)\n",
            "            Model Class: 'RandomForestClassifier'\n",
            "            Validation Score: 0.8176\n",
            "            \n",
            "\n",
            "            Vectorizer Class: 'CountVectorizer'\n",
            "            N-gram Range: (1, 4)\n",
            "            Model Class: 'LogisticRegression'\n",
            "            Validation Score: 0.8122\n",
            "            \n",
            "\n",
            "            Vectorizer Class: 'CountVectorizer'\n",
            "            N-gram Range: (1, 4)\n",
            "            Model Class: 'LinearSVC'\n",
            "            Validation Score: 0.8112\n",
            "            \n",
            "\n",
            "            Vectorizer Class: 'CountVectorizer'\n",
            "            N-gram Range: (1, 4)\n",
            "            Model Class: 'RandomForestClassifier'\n",
            "            Validation Score: 0.8303\n",
            "            \n",
            "\n",
            "            Vectorizer Class: 'TfidfVectorizer'\n",
            "            N-gram Range: (1, 4)\n",
            "            Model Class: 'LogisticRegression'\n",
            "            Validation Score: 0.7766\n",
            "            \n",
            "\n",
            "            Vectorizer Class: 'TfidfVectorizer'\n",
            "            N-gram Range: (1, 4)\n",
            "            Model Class: 'LinearSVC'\n",
            "            Validation Score: 0.8107\n",
            "            \n",
            "\n",
            "            Vectorizer Class: 'TfidfVectorizer'\n",
            "            N-gram Range: (1, 4)\n",
            "            Model Class: 'RandomForestClassifier'\n",
            "            Validation Score: 0.8134\n",
            "            \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRkL0mWyrZ4k",
        "outputId": "3cfd23cd-8c67-4014-8d57-fdb548654b7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "top_3_scores = sorted(results.keys(), reverse=True)[:3]\n",
        "\n",
        "for score in top_3_scores:\n",
        "  print('Score: {score}\\nParameters: {parameters}\\n'.format(\n",
        "        score=score, parameters=results[score]))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Score: 0.8373812038014784\n",
            "Parameters: {'vectorizer_class': <class 'sklearn.feature_extraction.text.CountVectorizer'>, 'ngram_range': (1, 1), 'model_class': <class 'sklearn.linear_model._logistic.LogisticRegression'>}\n",
            "\n",
            "Score: 0.8326403326403327\n",
            "Parameters: {'vectorizer_class': <class 'sklearn.feature_extraction.text.TfidfVectorizer'>, 'ngram_range': (1, 1), 'model_class': <class 'sklearn.svm._classes.LinearSVC'>}\n",
            "\n",
            "Score: 0.8316532258064516\n",
            "Parameters: {'vectorizer_class': <class 'sklearn.feature_extraction.text.CountVectorizer'>, 'ngram_range': (1, 2), 'model_class': <class 'sklearn.ensemble._forest.RandomForestClassifier'>}\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bY2qRxv6s2mL"
      },
      "source": [
        "**Train final model**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blpw09rMsq8D"
      },
      "source": [
        "top_score_key = top_3_scores[0]"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dm7sRRIatCu-",
        "outputId": "d4adfe7a-c0d4-4f59-e98e-748975951136",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "vectorizer_class = results[top_score_key]['vectorizer_class']\n",
        "ngram_range = results[top_score_key]['ngram_range']\n",
        "model_class = results[top_score_key]['model_class']\n",
        "\n",
        "# fit vectorizer\n",
        "vectorizer = vectorizer_class(analyzer='word', ngram_range=ngram_range, stop_words='english', max_features=50000)\n",
        "vectorizer.fit(training_data['cleaned_comment'])\n",
        "\n",
        "# transform text\n",
        "X = vectorizer.transform(training_data['cleaned_comment'])\n",
        "y = training_data['Insult']\n",
        "\n",
        "# fit model on training data\n",
        "model = model_class()\n",
        "model.fit(X, y)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THfEDDaFtgCa"
      },
      "source": [
        "**Run it live**\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AeBv9EUjtdlW"
      },
      "source": [
        "while True:\n",
        "    input_string = input('Please enter a string: ')\n",
        "    input_string = clean_text(input_string)\n",
        "    x_test = vectorizer.transform([input_string])\n",
        "    \n",
        "    prediction = model.predict(x_test)[0]\n",
        "    print('Insult?: {}'.format( bool(prediction)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvU2nVvUttxh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}